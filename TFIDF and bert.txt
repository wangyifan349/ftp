TF-IDF 是基于词项统计的稀疏线性表示。设 f(t,d) 为词 t 在文档 d 中的原始计数（或归一化计数），N 为语料库中文档总数，n(t) 为包含词 t 的文档数，常见公式为
TF(t,d) = f(t,d) / max_{t'} f(t',d)（或直接使用 f(t,d)），
IDF(t) = log(1 + N / (1 + n(t)))（或常见的平滑变体 log(N/(1+n(t))) + 1），
TFIDF(t,d) = TF(t,d) * IDF(t)。
结果是维度等于词表大小的稀疏向量，分量直接对应具体词的权重。优点：计算简单、可解释（每个维度对应词权重）、适合基于关键词的检索和快速相似度计算。缺点：假设词项独立，忽略词序和上下文，同形异义词不能区分，对短文本和词形变体鲁棒性差，无法捕捉短语或句法结构。

BERT 特征向量来自预训练的双向 Transformer 编码器，是稠密、上下文敏感的表示。输入为标记序列 x = (x1,...,xm)（含位置编码和子词分割），每层通过多头自注意力和前馈网络变换，注意力计算为
Attention(Q,K,V) = softmax(Q K^T / √d_k) V,
其中 Q,K,V 是由上一层隐藏状态经线性映射得到的查询/键/值矩阵。经 L 层后得到每个位置的隐藏向量 h^{(L)}_i；句子级向量常取 h^{(L)}_{[CLS]} 或对位置向量池化。BERT 表示能够捕捉词的上下文依赖、语义和句法信息，同一词在不同上下文会有不同向量，因而在语义匹配、问答和分类等下游任务上通常优于 TF-IDF。缺点：计算与内存开销大（自注意力的复杂度约为 O(m^2 · d)），需要预训练或微调，向量不易逐维解释为具体词权重，存储和推理成本远高于稀疏的 TF-IDF。
